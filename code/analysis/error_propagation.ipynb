{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error propagation with MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import datetime\n",
    "# Our numerical workhorses\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.special\n",
    "# Library to perform MCMC runs\n",
    "import emcee\n",
    "\n",
    "import mwc_induction_utils as mwc\n",
    "\n",
    "# Useful plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import corner\n",
    "\n",
    "# favorite Seaborn settings for notebooks\n",
    "rc={'lines.linewidth': 2, \n",
    "    'axes.labelsize' : 16, \n",
    "    'axes.titlesize' : 18,\n",
    "    'axes.facecolor' : 'F4F3F6',\n",
    "    'axes.edgecolor' : '000000',\n",
    "    'axes.linewidth' : 1.2,\n",
    "    'xtick.labelsize' : 13,\n",
    "    'ytick.labelsize' : 13,\n",
    "    'grid.linestyle' : ':',\n",
    "    'grid.color' : 'a6a6a6'}\n",
    "sns.set_context('notebook', rc=rc)\n",
    "sns.set_style('darkgrid', rc=rc)\n",
    "sns.set_palette(\"deep\", color_codes=True)\n",
    "\n",
    "# Magic function to make matplotlib inline; other style specs must come AFTER\n",
    "%matplotlib inline\n",
    "\n",
    "# This enables SVG graphics inline (only use with static plots (non-Bokeh))\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# Generate a variable with the day that the script is run\n",
    "today = str(datetime.datetime.today().strftime('%Y%m%d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the problem\n",
    "\n",
    "In our original parameter estimation [using MCMC](https://github.com/RPGroup-PBoC/mwc_induction/blob/master/code/analysis/MCMC_parameter_estimation.ipynb) reported a credible region for the MWC parameters and the fold-change based on the data and the fit to the model. But these estimates didn't include previous characterized uncertainty in the parameters of the model.\n",
    "\n",
    "For example at the time we computed the fold-chage for each of the RBS mutants we assumed we knew with 100% certainty the mean repressor copy number. This assumption is far from the truth since in their paper [Garcia and Phillips](http://www.pnas.org/content/108/29/12173.abstract) report the mean $\\pm$ standard deviation of the repressor copy number as revealed by multiple measurements of these quantities. The same applies to the repressor binding energies.\n",
    "\n",
    "The question then becomes how do we include these sources of uncertainty into our fold-change model with induction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes theorem as a method for learning\n",
    "\n",
    "The Bayesian framework gives a natural form on how to solve this issue. Let's focus first on the uncertainty of the repressor. We could write our parameter estimation using Bayes theorem as\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\epsilon_A, \\epsilon_I, R \\mid D, I) \\propto P(D \\mid \\epsilon_A, \\epsilon_I, R, I) \\cdot P(\\epsilon_A, \\epsilon_I, R \\mid I),\n",
    "\\end{equation}\n",
    "\n",
    "where we explicitly included the dependence on the repressor copy number. The second term on the right hand side, the so-called prior, **includes all the information we know before performing the experiment**. For simplicity we can assume that the parameters are independent such that we can rewrite this term as\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\epsilon_A, \\epsilon_I, R \\mid I) = P(\\epsilon_A \\mid I) \\cdot P(\\epsilon_I \\mid I) \\cdot P(R \\mid I).\n",
    "\\end{equation}\n",
    "\n",
    "Usually one doesn't want to bias the inference with the prior, therefore we are train to use *maximally uniformative priors* for the parameters. So for our previous model we chose a Jeffreys' prior for the $\\epsilon_A$ and $\\epsilon_I$ parameteres, i.e.\n",
    "\n",
    "\\begin{align}\n",
    "P(\\epsilon_A \\mid I) &\\propto \\frac{1}{\\epsilon_A}, \\\\\n",
    "P(\\epsilon_I \\mid I) &\\propto \\frac{1}{\\epsilon_I}\n",
    "\\end{align}\n",
    "\n",
    "But in a sense Bayes theorem is a *model for learning*. What we mean with that is that it naturally allows us to update our parameter estimates after performing an experiment and obtaining data. So in the case when one doesn't know anything at all about the parameter value these uniformative priors are the right thing to use. Now, what happens when one does indeed have information about the value of some of these parameters from previous experiments? Well, then that prior information should be included in the prior!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior on the parameters to include sources of uncertainty\n",
    "\n",
    "For the term $P(R \\mid I)$ we should include the uncertainty that Garcia and Phillips reported in their paper. Quoting the paper we find\n",
    "> Immunoblots were used to measure the number of Lac repressors in six strains with different constitutive levels of Lac repressor. Each value corresponds to an average of cultures grown on at least 3 different days. The error bars are the SD of these measurements.\n",
    "\n",
    "This means that the number they report is the *mean repressor copy number* and the *standard deviation on these measurements*. It is important to clarify that this standard deviation **does not** reflect the single-cell variability of repressors, but the experimental uncertainty when measuring the mean repressor copy number. In other words this standard deviation captures the lack of perfect accuracy when measuring this parameter, not the natural biological noise one expects on a clonal population.\n",
    "\n",
    "The repressor copy number per cell itself is a *discrete variable*, therefore one could naively think that the prior should be therefore given by a discrete distribution. But we should recall that what Garcia and Phillips measured with their immunoblots was not a single cell measurement, but a bulk measurement where they got to measure the **mean repressor copy number** which is not a discrete variable itself. As a matter of fact by the central limit theorem we know that we expect the distribution of the mean to be Gaussian. Therefore we can write\n",
    "\n",
    "\\begin{equation}\n",
    "P(R \\mid \\sigma_R, I) = \\frac{1}{\\sqrt{2 \\pi \\sigma_R^2}}\\exp \\left[ \\frac{(R - R^*)^2}{2 \\sigma_R^2}  \\right],\n",
    "\\end{equation}\n",
    "\n",
    "where $\\sigma_R$ is the characterized standard deviation that Garcia and Phillips experimentally characterized for each RBS mutant, and $R^*$ is the mean repressor level also characterized experimentally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we include this prior for the repressor copy number and an equivalent one for the repressor binding energy, and allow the MCMC walkers to walk on these two new dimensions while fitting the MWC parameters, then the posterior probability for the fold-change would include all of the characterized uncertainty on the parameters! In this way we can properly build the credible region for our predictions.\n",
    "\n",
    "So let's code this up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
